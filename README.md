# Transformer in visoin :
-
   The research demonstrates that Transformers, typically used in natural language processing, can be effectively applied to image recognition tasks. 
   This shifts the paradigm from relying heavily on convolutional neural networks (CNNs) to using a pure Transformer architecture, which can simplify 
   model design and potentially reduce the need for complex CNN structures.
-
   The authors mention a model created by Cordonnier , This model takes small sections of an image, called "patches," that are 2 Ã— 2 pixels in size. Think of a patch like a 
   tiny square cut out from a larger picture. If you imagine a picture made up of tiny squares, each square is a patch. The model then applies something called full self- 
   attention on these patches.
-
   The Vision Transformer model takes an image, breaks it into smaller pieces, translates those pieces into numbers, adds information about their positions, and processes them 
   through a special model to classify the image. This innovative approach allows the model to learn effectively and make accurate predictions about images.
# What is Self-Attention?
  Self-attention is a technique used in machine learning, especially in models called Transformers. It helps the model focus on different parts of the input data. For example, 
  in a sentence, it allows the model to pay attention to important words while ignoring less important ones.
  
